[{"title":"c++ 小技巧汇总","date":"2017-06-15T02:59:08.000Z","path":"2017/06/15/cxx/cpp-small-skills/","text":"如何获取.cpp文件行号与文件名先介绍几个编译器内置的宏定义，这些宏定义不仅可以帮助我们完成跨平台的源码编写，灵活使用也可以巧妙地帮我们输出非常有用的调试信息。ANSI C标准中有几个标准预定义宏（也是常用的）： LINE：在源代码中插入当前源代码行号； FILE：在源文件中插入当前源文件名； DATE：在源文件中插入当前的编译日期 TIME：在源文件中插入当前编译时间； STDC：当要求程序严格遵循ANSI C标准时该标识被赋值为1； __cplusplus：当编写C++程序时该标识符被定义。 123456789101112#include &lt;stdio.h&gt; int main() &#123; char file[16]; char func[16]; int line; sprintf(file,__FILE__); //文件名 sprintf(func,__FUNCTION__); //函数名 printf(\"file=%s\\n\",file); printf(\"func=%s\\n\",func); printf(\"%05d\\n\",__LINE__); //行号 return 0; &#125; 如何跨平台编写操作系统判定： Windows: WIN32 Linux: linux Solaris: __sun编译器判定： VC: _MSC_VER GCC/G++: GNUC SunCC: SUNPRO_C 和 SUNPRO_CC 123456789101112131415161718#include &lt;iostream&gt;using namespace std;void print1()&#123; cout &lt;&lt; \"this is window\" &lt;&lt; endl;&#125;void print2()&#123; cout &lt;&lt; \"this is linux\" &lt;&lt; endl;&#125;int main() &#123;#ifdef WIN32 print1();#elif linux print1();#else cout &lt;&lt; \"unknown os\" &lt;&lt; endl;#endif return 0;&#125;","tags":[{"name":"c++","slug":"c","permalink":"http://starsci.cn/tags/c/"}]},{"title":"轻量级数据库的性能比较","date":"2017-06-01T01:04:28.000Z","path":"2017/06/01/tools/benchmark-lighted-database/","text":"转自：Benchmarking LevelDB vs. RocksDB vs. HyperLevelDB vs. LMDB Performance for InfluxDB For quite some time we’ve wanted to test the performance of different storage engines for our use case with InfluxDB. We started off using LevelDB because it’s what we had used on earlier projects and RocksDB wasn’t around yet. We’ve finally gotten around to running some basic tests against a few different engines. Going forward it looks like RocksDB might be the best choice for us. However, we haven’t had the time to tune any settings or refactor things to take advantage of specific storage engine characteristics. We’re open to suggestions so read on for more detail. Before we get to results, let’s look at the test setup. We used a Digital Ocean droplet with 4GB RAM, 2 Cores, and 60GB of SSD storage. The next release of InfluxDB has a clearly defined interface for adding different storage engines. You’ll be able to choose LevelDB, RocksDB, HyperLevelDB, or LMDB. Which one you use is set through the configuration file. Under the covers LevelDB is a Log Structured Merge Tree while LMDB is a mmap copy on writeB+Tree. RocksDB and HyperLevelDB are forks of the LevelDB project that have different optimizations and enhancements. Our tests used a benchmark tool that isolated the storage engines for testing. The test does the following: Write N values where the key is 24 bytes (3 ints) Query N values (range scans through the key space in ascending order and does compares to see if it should stop) Delete N/2 values Run compaction Query N/2 values Write N/2 values At various steps we checked what the on disk size of the database was. We went through multiple runs writing anywhere from 1 million to 100 million values. Which implementation came out on top differed depending on how many values were in the database. For our use case we want to test on databases that have more values rather than less so we’ll focus on the results for the biggest run. We’re also not benchmarking put operations on keys that already exist. It’s either inserts or deletes, which is almost always the use case with time series data. The keys consist of 3 unsigned integers that are converted into big endian bytes. The first is an id that would normally represent a time series column id, the second is a time stamp, and the third is a sequence number. The benchmark simulates values written into a number of different ids (the first 8 bytes) and increasing time stamps and sequence numbers. This is a common load pattern for InfluxDB. Single points written to many series or columns at a time. Writes during the test happen in batches of 1,000 key/value pairs. Each key/value pair is a different series column id up to the number of series to write in the test. The value is a serialized protobuf object. Specifically, it’s a FieldValue with an int64 set. Here are the results of a run on 100 million values spread out over 500k columns: A few interesting things come out of these results. LevelDB is the winner on disk space utilization, RocksDB is the winner on reads and deletes, and HyperLevelDB is the winner on writes. On smaller runs (30M or less), LMDB came out on top on most of the metrics except for disk size. This is actually what we’d expect for B-trees: they’re faster the fewer keys you have in them. I’ve marked the LMDB compaction time as a loser in red because it’s a no-op and deletes don’t actually reclaim disk space. On a normal database where you’re continually writing data, this is ok because the old pages get used up. However, it means that the DB will ONLY increase in size. For InfluxDB this is a problem because we create a separate database per time range, which we call a shard. This means that after a time range has passed, it probably won’t be getting any more writes. If we do a delete, we need some form of compaction to reclaim the disk space. On disk space utilization, it’s no surprise that the Level variants came out on top. They compress the data in blocks while LMDB doesn’t use compression. Overall it looks like RocksDB might be the best choice for our use case. However, there are lies, damn lies, and benchmarks. Things can change drastically based on hardware configuration and settings on the storage engines. We tested on SSD because that’s where things are going (if not already there). Rocks won’t perform as well on spinning disks, but it’s not the primary target hardware for us. You could also potentially create a configuration with smaller shards and use LMDB for screaming fast performance. Here’s a gist of more of the results from different benchmark runs. We’re open to updating settings, benchmarks, or adding new storage engines. In the meantime we’ll keep iterating and try to get to the best possible performance for the use case of time series data.","tags":[{"name":"database","slug":"database","permalink":"http://starsci.cn/tags/database/"},{"name":"benchmark","slug":"benchmark","permalink":"http://starsci.cn/tags/benchmark/"}]},{"title":"泛型编程","date":"2017-05-02T01:04:25.000Z","path":"2017/05/02/cxx/cplusplus-generic-programming/","text":"最近在学习算法导论，顺便温习一下以前的C++概念，确实很久很久没有接触过C++了。现在回头重新看这些知识点发现自己真的是只掌握了一丢丢的皮毛而已。这部分是关于STL的系列，回头将这些内容慢慢补齐。 现代C++中STL有六大组件： 容器 配置器 迭代器 算法 配接器 仿函数 迭代器一般而言，我们针对数据抽象出其中的共性，构建成数据结构来表示一个通用的数据概念。而泛型编程是通过抽象可重用的算法来表示一种通用方法。简单说，链表、数组、队列等等都具有不同的数据结构，但遍历的操作、读取某一元素的操作往往是相似的，这时候我们利用迭代器iterator来完成。 简单来说，迭代器就是一个可控的指针，可以有一些列的操作方式： 为了能够对迭代器执行解引用操作，访问其指向的元素，需要定义*p 为了能够将一个迭代器的指向位置赋值给另一个，需要定义p=q 为了能够实现比较操作，需要定义p==q或者p!=q 为了能够实现遍历操作，至少需要定义++p或者p++操作 for(:)的实现对于for(:)是C++11的新语法，我们可以通过实现迭代器来实现对自定义数据结构的循环遍历。为实现for(:)我们需要定义以下： begin()和end()获取第一个元素的指针和超尾指针。 重载++操作以满足遍历需求。 重载!=操作以在达到end()时终止操作。 重载*操作以读取该元素的值。 123456789101112131415161718192021222324252627282930313233343536template&lt;typename T&gt;class DLinkNode &#123;public: T data; DLinkNode *prev; DLinkNode *next;&#125;;template&lt;typename T&gt;class DLinkList &#123;public: class iterator &#123; public: iterator(); iterator(DLinkNode&lt;T&gt; *s); iterator(const iterator &amp;s); DLinkNode&lt;T&gt; operator*() const; iterator&amp; operator++(); bool operator!=(const iterator&amp; p); private: DLinkNode&lt;T&gt; *_cur; &#125;;public: DLinkList(); DLinkList(const DLinkList&amp; s); ~DLinkList(); // iterator iterator begin(); iterator end();protected: DLinkNode&lt;T&gt; *_head;&#125;; 首先我们定义了一个双向链表(这里省去了双向链表的插入、删除等操作)，只是一个结构上的示意。iterator定义为嵌套类或内部类。下面是一些内部实现的具体细节：1234template&lt;typename T&gt;inline typename DLinkList&lt;T&gt;::iterator DLinkList&lt;T&gt;::begin() &#123; return typename DLinkList&lt;T&gt;::iterator(_head);&#125; 这里需要注意的是，类iterator是从属于类DlinkList&lt;T&gt;，因此在类外需要用typename关键词声明该类型为嵌套从属类型。 完整的实现可以访问：https://github.com/atranitell/startup/blob/master/startup/structure/doubly_linked_list.hpp","tags":[{"name":"c++","slug":"c","permalink":"http://starsci.cn/tags/c/"},{"name":"STL","slug":"STL","permalink":"http://starsci.cn/tags/STL/"}]},{"title":"机器学习常用算法盘点","date":"2017-04-28T00:52:57.000Z","path":"2017/04/28/machine-learning/machine-learning-common-summary/","text":"翻译来源：http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/ 在本文中，我将提供两种分类机器学习算法的方法。一是根据学习方式分类，二是根据类似的形式或功能分类。这两种方法都很有用，不过，本文将侧重后者，也就是根据类似的形式或功能分类。在阅读完本文以后，你将会对监督学习中最受欢迎的机器学习算法，以及它们彼此之间的关系有一个比较深刻的了解。 从学习方式分类算法对一个问题建模的方式很多，可以基于经历、环境，或者任何我们称之为输入数据的东西。机器学习和人工智能的教科书通常会让你首先考虑算法能够采用什么方式学习。实际上，算法能够采取的学习方式或者说学习模型只有几种，下面我会一一说明。对机器学习算法进行分类是很有必要的事情，因为这迫使你思考输入数据的作用以及模型准备过程，从而选择一个最适用于你手头问题的算法。 监督学习：输入数据被称为训练数据，并且每一个都带有标签，比如“广告/非广告”，或者当时的股票价格。通过训练过程建模，模型需要做出预测，如果预测出错会被修正。直到模型输出准确的结果，训练过程会一直持续。常用于解决的问题有分类和回归。常用的算法包括逻辑回归和BP神经网络。 无监督学习：输入数据没有标签，输出没有标准答案，就是一系列的样本。无监督学习通过推断输入数据中的结构建模。这可能是提取一般规律，可以是通过数学处理系统地减少冗余，或者根据相似性组织数据。常用于解决的问题有聚类、降维和关联规则的学习。常用的算法包括 Apriori 算法和 K 均值算法。 半监督学习：半监督学习的输入数据包含带标签和不带标签的样本。半监督学习的情形是，有一个预期中的预测，但模型必须通过学习结构整理数据从而做出预测。常用于解决的问题是分类和回归。常用的算法是所有对无标签数据建模进行预测的算法（即无监督学习）的延伸。 从功能角度分类研究人员常常通过功能相似对算法进行分类。例如，基于树的方法和基于神经网络的方法。这种方法也是我个人认为最有用的分类方法。不过，这种方法也并非完美，比如学习矢量量化（LVQ），就既可以被归为神经网络方法，也可以被归为基于实例的方法。此外，像回归和聚类，就既可以形容算法，也可以指代问题。 回归算法：回归分析是研究自变量和因变量之间关系的一种预测模型技术。这些技术应用于预测时间序列模型和找到变量之间关系。回归分析也是一种常用的统计学方法，经由统计机器学习融入机器学习领域。“回归”既可以指算法也可以指问题，因此在指代的时候容易混淆。实际上，回归就是一个过程而已。常用的回归算法包括：普通最小二乘回归（OLSR），线性回归，逻辑回归，逐步回归，多元自适应回归样条法（MARS），局部估计平滑散点图（LOESS） 基于实例的学习算法：基于实例的学习通过训练数据的样本或事例建模，这些样本或事例也被视为建模所必需的。这类模型通常会建一个样本数据库，比较新的数据和数据库里的数据，通过这种方式找到最佳匹配并做出预测。换句话说，这类算法在做预测时，一般会使用相似度准则，比对待预测的样本和原始样本之间的相似度，再做出预测。因此，基于实例的方法也被称之为赢家通吃的方法（winner-take-all）和基于记忆的学习（memory-based learning）。常用的基于实例的学习算法包括：k-邻近算法（kNN），学习矢量量化算法（LVQ），自组织映射算法（SOM），局部加权学习算法（LWL） 正则化算法：正则化算法背后的思路是，参数值比较小的时候模型更加简单。对模型的复杂度会有一个惩罚值，偏好简单的、更容易泛化的模型，正则化算法可以说是这种方法的延伸。我把正则化算法单独列出来，原因就是我听说它们十分受欢迎、功能强大，而且能够对其他方法进行简单的修饰。常用的正则化算法包括：岭回归，LASSO 算法，Elastic Net，最小角回归算法（LARS） 决策树算法：决策树算法的目标是根据数据属性的实际值，创建一个预测样本目标值的模型。训练时，树状的结构会不断分叉，直到作出最终的决策。也就是说，预测阶段模型会选择路径进行决策。决策树常被用于分类和回归。决策树一般速度快，结果准，因此也属于最受欢迎的机器学习算法之一。常用的决策树算法包括：分类和回归树（CART），ID3 算法，C4.5 算法和 C5.0 算法，CHAID 算法，单层决策树，M5 算法，条件决策树 贝叶斯算法：贝叶斯方法指的是那些明确使用贝叶斯定理解决分类或回归等问题的算法。常用的贝叶斯算法包括：朴素贝叶斯算法，高斯朴素贝叶斯算法，多项式朴素贝叶斯算法，AODE 算法，贝叶斯信念网络（BBN），贝叶斯网络（BN） 聚类算法：聚类跟回归一样，既可以用来形容一类问题，也可以指代一组方法。聚类方法通常涉及质心（centroid-based）或层次（hierarchal）等建模方式，所有的方法都与数据固有的结构有关，目标是将数据按照它们之间共性最大的组织方式分成几组。换句话说，算法将输入样本聚成围绕一些中心的数据团，通过这样的方式发现数据分布结构中的规律。常用的聚类算法包括：K-均值，K-中位数，EM 算法，分层聚类算法 关联规则学习：关联规则学习在数据不同变量之间观察到了一些关联，算法要做的就是找出最能描述这些关系的规则，也就是获取一个事件和其他事件之间依赖或关联的知识。常用的关联规则算法有：Apriori 算法，Eclat 算法 人工神经网络：人工神经网络是一类受生物神经网络的结构及/或功能启发而来的模型。它们是一类常用于解决回归和分类等问题的模式匹配，不过，实际上是一个含有成百上千种算法及各种问题变化的子集。注意这里我将深度学习从人工神经网络算法中分离了出去，因为深度学习实在太受欢迎。人工神经网络指的是更加经典的感知方法。常用的人工神经网络包括：感知机，反向传播算法（BP 神经网络），Hopfield网络，径向基函数网络（RBFN） 深度学习算法：深度学习算法是人工神经网络的升级版，充分利用廉价的计算力。近年来，深度学习得到广泛应用，尤其是语音识别、图像识别。深度学习算法会搭建规模更大、结构更复杂的神经网络，正如上文所说，很多深度学习方法都涉及半监督学习问题，这种问题的数据一般量极大，而且只有很少部分带有标签。常用的深度学习算法包括：深度玻尔兹曼机（DBM），深度信念网络（DBN），卷积神经网络（CNN），栈式自编码算法（Stacked Auto-Encoder） 降维算法：降维算法和聚类有些类似，也是试图发现数据的固有结构。但是，降维算法采用的是无监督学习的方式，用更少（更低维）的信息进行总结和描述。降维算法可以监督学习的方式，被用于多维数据的可视化或对数据进行简化处理。很多降维算法经过修改后，也被用于分类和回归的问题。常用的降维算法包括：主成分分析法（PCA），主成分回归（PCR），偏最小二乘回归（PLSR），萨蒙映射，多维尺度分析法（MDS），投影寻踪法（PP），线性判别分析法（LDA），混合判别分析法（MDA），二次判别分析法（QDA），灵活判别分析法（Flexible Discriminant Analysis，FDA） 模型融合算法：模型融合算法将多个简单的、分别单独训练的弱机器学习算法结合在一起，这些弱机器学习算法的预测以某种方式整合成一个预测。通常这个整合后的预测会比单独的预测要好一些。构建模型融合算法的主要精力一般用于决定将哪些弱机器学习算法以什么样的方式结合在一起。模型融合算法是一类非常强大的算法，因此也很受欢迎。常用的模型融合增强方法包括：Boosting，Bagging，AdaBoost，堆叠泛化（混合），GBM 算法，GBRT 算法，随机森林 其他：还有很多算法都没有涉及。例如，支持向量机（SVM）应该被归为哪一组？还是说它自己单独成一组？我还没有提到的机器学习算法包括：特征选择算法，Algorithm accuracy evaluation，Performance measures","tags":[{"name":"machine-learning","slug":"machine-learning","permalink":"http://starsci.cn/tags/machine-learning/"}]},{"title":"c++中的内存分配方式","date":"2017-04-21T06:17:23.000Z","path":"2017/04/21/cxx/cplus-plus-memory-allocate/","text":"一个由C/C++编译程序占用的内存分为以下几个部分： 栈区 stack: 由编译器自动分配释放，存放函数的参数值，局部变量值等。 堆区 heap: 由程序员分配释放。 全局静态区 static: 全局变量和静态变量存储是放在一起的，初始化全局变量和静态变量在一块区域，未初始化的全局变量和静态变量在另一块区域。分别是data区和bbs区。 文字常量区: 常量字符串就放在这里，程序结束后系统统一释放comment区。 程序代码区: 存放函数体二进制代码code区。 函数栈的空间分配方式1234567891011void test(int a, int b, int c, int d) &#123; int e = a; int f = b; int g = c; int h = d;&#125;int main() &#123; test(1, 2, 3, 4); return 0;&#125; 上面是一段非常简单的C++代码，现在来详细解释一下函数的内存分配问题。在main()函数中调用test(1, 2, 3, 4)函数。在这个过程中，参数以此逆序压入栈中。 push 4 // 010A1A8E push 3 // 010A1A90 push 2 // 010A1A92 push 1 // 010A1A94 call func_addr // 010A1A96 -&gt; 010A11D6 那么010A11D6是哪里呢，实际上在这是一个函数表，如下：12345678910_RaiseException@16:010A11CC jmp _RaiseException@16 (010A4DFAh) ___raise_securityfailure:010A11D1 jmp __raise_securityfailure (010A48E0h) test:010A11D6 jmp test (010A17F0h) ___report_securityfailureEx:010A11DB jmp __report_securityfailureEx (010A4B90h) __vcrt_va_start_verify_argument_type&lt;char const * const&gt;:010A11E0 jmp __vcrt_va_start_verify_argument_type&lt;char const * const&gt; (010A22A0h) 很明显，函数表里储存了test函数的具体入口地址010A17F0，继续分析。12345678910111213141516171819202122232425262728293031void test(int a, int b, int c, int d) &#123;008217F0 push ebp // 备份调用方EBP指针008217F1 mov ebp,esp // 建立被调用方的栈底008217F3 sub esp,0F0h // 为局部变量分配空间008217F9 push ebx 008217FA push esi 008217FB push edi 008217FC lea edi,[ebp-0F0h] 00821802 mov ecx,3Ch 00821807 mov eax,0CCCCCCCCh 0082180C rep stos dword ptr es:[edi] int e = a;0082180E mov eax,dword ptr [a] // 注意这里是利用数组索引 int e = a;00821811 mov dword ptr [e],eax int f = b;00821814 mov eax,dword ptr [b] 00821817 mov dword ptr [f],eax int g = c;0082181A mov eax,dword ptr [c] 0082181D mov dword ptr [g],eax int h = d;00821820 mov eax,dword ptr [d] 00821823 mov dword ptr [h],eax &#125;00821826 pop edi 00821827 pop esi 00821828 pop ebx 00821829 mov esp,ebp 0082182B pop ebp 0082182C ret 回到主函数，继续执行：100C21A9B add esp,10h // 释放参数空间，恢复调用前的函数栈 变量的存储机制1234567891011121314#include &lt;string&gt;int a=0; // 全局初始化区char *p1; // 全局未初始化区void main()&#123; int b; // 栈 char s[]=\"abc\"; // 栈 char *p2; // 栈 char *p3=\"123456\"; // 123456\\0在常量区，p3在栈上。 static int c=0; // 全局（静态）初始化区 p1 = (char*)malloc(10); p2 = (char*)malloc(20); // 分配得来的10和20字节的区域就在堆区。 strcpy(p1,\"123456\"); // 123456\\0放在常量区，编译器可能会将它与p3所向\"123456\\0\"优化成一个地方。&#125;","tags":[{"name":"c++","slug":"c","permalink":"http://starsci.cn/tags/c/"},{"name":"memory","slug":"memory","permalink":"http://starsci.cn/tags/memory/"}]},{"title":"Softmax vs. Sotmax-Loss 数值稳定性","date":"2017-04-17T02:09:14.000Z","path":"2017/04/17/deep-learning/softmax-n-softmax-loss/","text":"最近一直在和学校的老师做Deep Learning有关的课题，虽然说是在做学术上的事情，可是大多数仍然是工程方面的。现在Deep Learning的一个很大的趋势是工程化。虽然学术界有很多看起来很漂亮的理论和方法，但在实际使用中，用的往往不多，反倒是经过历史检验的那么几种简单朴素的方法却是用途最为广泛。这也是一件挺有意思的事情，越复杂的问题反倒需要越简单的想法去解决，比如SGD，Dropout，BN等等，都不是很复杂的模型，但却是用途最为广泛的技巧。另一个方面是，算法的变革所提升的精度远远不如数据集的扩增，网络的增大收益来的快和简单。因此，工业界往往喜欢boost网络的训练速度，增大数据集，然后不断的调参。 只是最近的一些感慨，我还是蛮喜欢数学上精巧的东西。有意思的事情，前两天一个初中小女孩问我数学题，本来以为信手捏来，结果愣生生把一道简单的集合题做成了解析几何，还需要求高次方程，汗颜。不要问我结果，我已经让那个孩子去问老师了，回来给我讲一讲。这两天把以前保存的文章拿出来，好好咀嚼一下。过去看到一篇好的文章总爱收藏起来却很久再也没有翻开过，似乎看了很多文章，实际上没有经历过真正的推敲和实践就是在自欺欺人。 Softmax 转载：http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/ Softmax一般会出现在网络的最后一层，用于输出目标类的概率分布，往往用于分类问题。它的定义如下：$ \\sigma(z)=(\\sigma_1(z),\\dots,\\sigma_m(z)) $$$ \\sigma_i(z)=\\frac{\\exp(z_i)}{\\sum_{j=1}^{m}\\exp(z_j)}, \\quad i=1,\\dots,m $$它在Logistic Regression里起到的作用是线性预测值转化为类别概率：假设 $z_i=w_i^T+b_i$ 是第 $i$ 个类别的线性预测结果，再将其归一化到 $(0, 1)$ 区间，现在每个 $o_i=\\sigma_i(z)$ 就可以解释成观察到的数据 $x$ 属于类别 $i$ 的概率，或者称作似然 (Likelihood)。 然后Logistic Regression的目标函数是根据最大似然原则来建立的，假设数据 $x$ 所对应的类别为 $y$ ，则根据我们刚才的计算最大似然就是要最大化 $o_y$ 的值(或者说，最小化 $-\\log(o_y)$ 也叫做negative log-likelihood)，如下：$$ l(y, o) = -\\log(o_y) $$ 而Softmax-loss就是将两者结合起来，把 $o_y$ 的定义展开即可：$$ \\tilde{l}(y,z) = -\\log\\left(\\frac{e^{z_y}}{\\sum_{j=1}^{m}e^{z_j}} \\right) = \\log\\left(\\sum_{j=1}^{m}e^{z_j}\\right) - z_y $$ 没有任何fancy的东西，比如如果我们要写一个Logistic Regression的solver，那么因为要处理的就是这个东西，比较自然地就可以将整个东西合在一起来考虑，或者甚至将$z_i=w_i^T+b_i$的定义直接一起带进去然后对$w$和$b$进行求导来得到Gradient Descent的update rule，例如我们之前介绍Gradient Descent的时候举的两类Logistic Regression的例子就是这样做的。 Back Prop假设存在一个三层的神经网络，如下图： 除了最开始的数据层 $L^0$ 之外，每一层都有输入节点和输出节点，我们用 $I_2^1$ 表示第一层的第二个输入节点，$O_3^1$ 表示第一层的第三个输出节点，每一层的输入和输出节点数量并不一定要一样多，但通常情况上一层的输出往往就是下一层结点输入的复制，比如 $I_3^2 = O_3^1$，因为所有的计算都是发生在每一层的内部。所以对普通的神经网络，通常每一层进行的计算都是一个线性映射再加一个activation function，例如sigmoid：$$ O_i^1 = S(\\sum_{j=1}^{3}w_{ij}^{1}I_{j}^{1}+b_i^1) = S\\left(\\left\\langle w_i^1, I^1 \\right\\rangle+b_i^1\\right) $$现在要对参数 $w_{ij}^1$ 进行求导计算它的gradient来进行gradient descent一类的参数优化，利用链式法则如下：$$ \\frac{\\partial{f}}{\\partial{w_{ij}^1}} = \\sum_{k=1}^{3}\\frac{\\partial{O_k^1}}{\\partial{w_{ij}^1}}\\cdot\\frac{\\partial{f}}{\\partial{O_k^1}} $$注意到 $ \\partial{O_k^1}/\\partial{w_{ij}^1} $ 这一项只是和网络的第一层结构有关，只需要知道该结构内部的参数即可计算，而后面一部分相当于 $ \\partial{f}/\\partial{O_k^1} = \\partial{f}/\\partial{I_k^2} $ 即这一层的输出等于下一层的输入。因此，后一项只和下一层的结构和参数有关，不需要知道任何第一层有关的信息。 因此整个网络的参数的gradient的计算方式是从顶层出发，在 $L^p$ 层的时候，会拿到从 $L^{p+1}$ 得到的$\\partial{f}/\\partial{I^{p+1}}$ 也就是 $\\partial{f}/{\\partial{O^p}}$， 然后需要做两个计算：一个是自己层内的参数的gradient，比如是一个不同的full connect layer，则会像上文那样计算，如果没有则继续向下传递。根据链式规则，我们可以计算：$$ \\frac{\\partial{f}}{\\partial{O_i^{p-1}}} = \\frac{\\partial{f}}{\\partial{I_i^p}} = \\sum_{k=1}^{n}\\frac{\\partial{O_k^p}}{\\partial{I_i^p}}\\cdot\\frac{\\partial{f}}{\\partial{O_k^p}} $$ Softmax-Loss搞清楚Back Prop之后让我们回到Softmax-Loss层，由于该层没有参数，我们只需要计算向后传递的导数就可以了，此外由于该层是最顶层，所以不使用链式法则就可以计算对于最终输出(loss)的导数。我们用 $\\tilde{l}(y,z)$ 表示，其中一个是true label $y$，一个是网络层的输出值 $z$。由于链式法则的缘故，我们只需要计算 $\\partial{\\tilde{l}}/\\partial{z}$ 然后丢给下一层即可。因此：$$ \\frac{\\partial\\tilde{l}(y,z)}{\\partial{z_k}} = \\frac{\\exp(z_k)}{\\sum_{j=1}^m\\exp(z_j)} - \\delta_{ky} = \\sigma_{k}(z)-\\delta_{ky} $$其中 $\\sigma_{k}(z)$ 是Softamx-Loss的中间步骤计算的结果，而 $\\delta_{ky}$，当 $k=y$ 时为 $1$，当 $k \\ne y$ 时为 $0$，其实就是是否作为变量(常数)对 $z_k$ 求导。","tags":[{"name":"cnn","slug":"cnn","permalink":"http://starsci.cn/tags/cnn/"}]},{"title":"c++ 模板类的使用","date":"2017-04-16T10:52:08.000Z","path":"2017/04/16/cxx/cplusplus-template-usage/","text":"今天实现了一个很简单的模板类。一开始的思路就和编写普通类一样，定义和实现分离。首先在calculate.h文件中定义了模板类类型。123456789// 在calculate.h文件中#pragma once#include &lt;iostream&gt;template &lt;typename T&gt;class Calculate &#123;public: T add(T a, T b);&#125;; 然后在calculate.cpp中定义实现。123456#include \"calculate.h\"template &lt;typename T&gt;T Calculate&lt;T&gt;::add(T a, T b) &#123; return a + b;&#125; 在主函数中调用。123456#include \"calculate.h\"int main() &#123; Calculate&lt;int&gt; c1; std::cout &lt;&lt; c1.add(5, 6) &lt;&lt; std::endl;&#125; 之后便是理所当然的报错。 error LNK2019: 无法解析的外部符号 “public: int __thiscall Calculate::add(int,int)” (?add@?$Calculate@H@@QAEHHH@Z)，该符号在函数 _main 中被引用fatal error LNK1120: 1 个无法解析的外部命令 解决方案：模板不支持分离编译, 把你模板类的声明和实现放到.h文件里面 C++的编译过程首先是c++编译的一个简单的过程： 一个编译单元（translation unit）指一个.cpp文件以及它所#include的所有.h文件 .h文件代码将被扩展并包含到.cpp文件。 该.cpp文件被编译器编译成.obj文件，该文件具备可执行属性，为二进制。因为不保证有main函数接口，所以不一定可以被执行。 工程内所有的.cpp文件被分离编译完成后，再由连接器（linker）进行连接成为一个.exe文件。 举例来说：1234567891011121314//---------------test.h-------------------//void f();//这里声明一个函数f//---------------test.cpp--------------//#include”test.h”void f() &#123;…//do something&#125; //这里实现出test.h中声明的f函数//---------------main.cpp--------------//#include”test.h”int main() &#123; f(); //调用f，f具有外部连接类型&#125; 在这个例子中，test. cpp和main.cpp各自被编译成不同的.obj文件（姑且命名为test.obj和main.obj），在main.cpp中，调用了f函数，然而当编译器编译main.cpp时，它所仅仅知道的只是main.cpp中所包含的test.h文件中的一个关于void f()的声明，所以，编译器将这里的f看作外部连接类型，即认为它的函数实现代码在另一个.obj文件中，本例也就是test.obj，也就是说，main.obj中实际没有关于f函数的哪怕一行二进制代码，而这些代码实际存在于test.cpp所编译成的test.obj中。在main.obj中对f的调用只会生成一行call指令，像这样call addr_f。 在编译时，这个call指令显然是错误的，因为main.obj中并无一行f的实现代码。那怎么办呢？这就是连接器的任务，连接器负责在其它的.obj中（本例为test.obj）寻找f的实现代码，找到以后将call addr_f这个指令的调用地址换成实际的f的函数进入点地址。需要注意的是：连接器实际上将工程里的.obj连接成了一个.exe文件，而它最关键的任务就是上面说的，寻找一个外部连接符号在另一个.obj中的地址，然后替换原来的虚假地址。 更细致的讲，call addr_f这行指令其实并不是这样的，它实际上是所谓的stub，也就是一个jmp 0xABCDEF。这个地址可能是任意的，然而关键是这个地址上有一行指令来进行真正的call addr_f动作。也就是说，这个.obj文件里面所有对f的调用都jmp向同一个地址。这样做的好处就是连接器修改地址时只要对后者的call XXX地址作改动就行了。但是，连接器是如何找到f的实际地址的呢（在本例中这处于test.obj中），因为.obj与.exe的格式是一样的，在这样的文件中有一个符号导入表和符号导出表（import table和export table）其中将所有符号和它们的地址关联起来。这样连接器只要在test.obj的符号导出表中寻找符号f（当然C++对f作了mangling）的地址就行了，然后作一些偏移量处理后（因为是将两个.obj文件合并，当然地址会有一定的偏移，这个连接器清楚）写入main.obj中的符号导入表中f所占有的那一项即可。 模板类的问题所在而对于模板类来说，举前面calculate代码的例子。由于C++标准明确表示当一个模板不被用到的时侯它就不该被实例化出来(因为编译器根本不知道要实例化为哪一种类型)，所以对calculate.cpp的编译不会生成任何代码。而对main.cpp编译时，发现存在add&lt;int&gt;函数，于是寄予希望给连接器，希望它能在其他的.obj文件中找到接口。而calculate.obj因为模板从来没有实例化过，所以就会提示链接错误。 如果将模板的定义和实现放到.h文件中，那么在main.cpp中就会就地展开，并实例化为Calculate&lt;int&gt;类，从而能够顺利进行调用。所以归其根本在于这种分离式编译的方法，导致我们必须要在.h中实现模板类的所有定义。 注：在boost中，能看到很多.hpp文件，该文件就是大量的模板类。","tags":[{"name":"c++","slug":"c","permalink":"http://starsci.cn/tags/c/"},{"name":"template","slug":"template","permalink":"http://starsci.cn/tags/template/"}]},{"title":"c++ 函数指针","date":"2017-04-14T14:48:20.000Z","path":"2017/04/14/cxx/cplusplus-function-pointer/","text":"函数对象也是一种变量，函数变量的值是函数的入口地址。函数指针是指向函数类型的指针。C++是强类型语言，因此该函数指针必须显示的声明其指向哪一类函数对象。决定函数对象的有两个部分，一个是返回值，一个是形参类型。 函数指针调用同类型的不同函数12345678910111213141516float add(float a, float b) &#123; return (a + b);&#125;float minus(float a, float b) &#123; return (a - b);&#125;void test() &#123; float (*p1)(float, float); float (*p2)(float, float); p1 = add; p2 = minus; std::cout &lt;&lt; p1(1.0, 2.0) &lt;&lt; std::endl; std::cout &lt;&lt; p2(1.0, 2.0) &lt;&lt; std::endl;&#125; 函数指针作为形参传递 - 接上面的代码123456789101112// use `using` to simplify the codeusing ops = float(*)(float, float);// surly, you also could use `float(*p)(float, float) to replace `ops p`float calculate(float a, float b, ops p) &#123; return p(a, b);&#125;void test() &#123; std::cout &lt;&lt; calculate(1.0, 2.0, add) &lt;&lt; std::endl; std::cout &lt;&lt; calculate(1.0, 2.0, minus) &lt;&lt; std::endl;&#125;","tags":[{"name":"c++","slug":"c","permalink":"http://starsci.cn/tags/c/"}]},{"title":"关于LIB和DLL的区别和使用","date":"2017-04-14T08:13:30.000Z","path":"2017/04/14/tools/visual-studio-lib-dll-usage/","text":"以前用visual studio c++写玩具的时候，就是单工程单项目，也很少使用外部库例如openCV等。但明显很多东西需要分开写，比如为某个算法提供的基础方法等。因此，需要分离项目。简单的说，一个入口程序.exe提供入口方法，调用不同的dll和lib库的实现运行产品。然而，以前一直没怎么弄懂dll和lib的区别，使用也是稀里糊涂的说不清。 以下内容部分来自：http://www.cppblog.com/amazon/archive/2009/09/04/95318.html 基础DLL库中记录了函数位置的入口地址，代码在运行时加载进进程空间。LIB库中包含了函数本身的实现，在编译时将代码加入到程序中。 LIB的使用在Visual Studio-&gt;属性-&gt;连接器-&gt;输入-&gt;附加依赖项，中添加lib文件。此外，为索引到文件地址，仍然需要在属性-&gt;VC++目录-&gt;库目录增加lib文件的路径，如果需要使用到头文件，还需要在包含目录中填充.h目录。","tags":[{"name":"vs","slug":"vs","permalink":"http://starsci.cn/tags/vs/"}]},{"title":"Visual Studio C++ 异常处理","date":"2017-04-14T08:05:05.000Z","path":"2017/04/14/tools/visual-studio-fatal-solution/","text":"最近遇到一系列的visual studio c++的编译问题，在这里将遇到的问题整理下来，以便未来遇到时能够进行参考。 Error: LNK2005 已经在 *.obj中定义，找到一个或多个多重定义的符号在c++的头文件中，应该只包含声明以及inline 函数, 模板类/函数模板的定义。如果在.h文件中实现了某个函数，并在多个.cpp中调用该头文件，会出现该问题。解决方法是，在一个.cpp中进行函数的实现，在.h文件中进行声明。 转载： http://blog.csdn.net/qq_32541007/article/details/51607291 问题在于没有把握清楚头文件的作用以及CPP之间函数与变量命名方式的规则, 头文件的内容： 类型(struct-第7章)及类(class-第8章)的声明 函数的声明 inline函数的定义与实现 符号常量的定义及常变量的定义 全局变量的extern声明 其它需要的头文件 cpp之间的全局变量不能同名，所以不能在头文件中定义一个变量后在两个cpp中都包含头文件，否则会报上述错误。解决方法是加const 限定，另外不同cpp之间的函数必须要有区分（函数名，参数类型）否则也会报错。要调用cpp之间变量或函数必须加申明。 关于头文件包含问题我现在见到两种方式避免重复包含。第一种是：1#pragma once 第二种是：1234#ifndef _QUICK_SORT_H_#define _QUICK_SORT_H_// your code block#endif 两种方式的区别是：第一种是微软独有的方式，可能在跨平台下不能正常编译未测试。第二种是传统从c时代演变过来的方法，可泛平台使用，但缺点也很明显，你的所有文件的宏声明都不能重复。 warning LNK4042: 对象被多次指定;已忽略多余的指定在资源管理器中把.h误生成.cpp，然后自己在资源管理器中改了名字，导致出现这个问题。解决方法是将提示错误的头文件删除，重新添加一份就好了。","tags":[{"name":"c++","slug":"c","permalink":"http://starsci.cn/tags/c/"},{"name":"vs","slug":"vs","permalink":"http://starsci.cn/tags/vs/"}]},{"title":"linux的相关配置命令","date":"2017-04-04T09:34:04.000Z","path":"2017/04/04/tools/linux-configuration-guide/","text":"虽然linux确实蛮好用的，如果熟悉命令的话。但是一直以来在windows下工作，因此很多命令以前挺熟悉的现在又忘记了，还有一些服务的配置等。现在做个memo记录一下，以后再进行配置的时候就可以进行参考。当然，本机的linux版本是ubuntu 16.04。 配置用户权限一直以来都不太熟悉用户权限的控制，直到开始配置网站和ftp的时候，才发现这个东西还是要仔细看看的。在linux下用数字来表示三种权限： 0 无权限 1 执行权限 2 写入权限 4 读取权限 利用chmod xxx filename/foldname改变文件或文件夹的权限。其中xxx按顺序分别代表owner, group, others的权限配置。 配置 vsftp12apt install vsftpdvim /etc/vsftpd.conf # config vsftp 默认是不允许使用root用户登陆，因此还需要增加一个ftp用户。 安装 node一般来说，直接使用apt install nodejs npm就可以了，但是ubuntu 16.04默认并不是最新的。因此很多插件不支持，比如hexo-generator-json-content。123apt purge nodejs npmcurl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -sudo apt-get install -y nodejs 关于sudo后，环境变量的改变有时候，我们安装程序时必须要使用sudo，然而sudo安装后增加的path和普通模式下是不一样的。如果使用软连接的话，又会带来一大堆版本问题。这个时候，我们可以修改普通用户的.bash_profile文件:1234$vi ~/.bash_profile# 添加:PATH=$PATH:$HOME/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbinsource ~/.bash_profile 另外一种情况是，执行sudo命令时，发现command not found。我们需要增加本地路径到sudoers文件中。123vim /etc/sudoers# find secure_path and add lineDefaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin","tags":[{"name":"linux","slug":"linux","permalink":"http://starsci.cn/tags/linux/"}]},{"title":"Linux进程后台运行的几种方式","date":"2017-04-04T07:05:32.000Z","path":"2017/04/04/tools/run-on-the-backend-in-linux/","text":"最近在aliyun上配置个人网站。有一些用node生成出静态网页，用nginx提供路由服务。另一些采用node原生服务，比如ejs等开启在另外的端口。对于前者已经注册成系统服务，可以随系统后台开启，但对于后者，如果关闭session便会中断服务。个人推荐采用screen方式进行后台处理。 主要借鉴于：Linux 进程后台运行的几种方式 Ctrl+z/bg/nohup/setsid/&amp;在Linux中，如果要让进程在后台运行，一般情况下，我们在命令后面加上&amp;即可，实际上，这样是将命令放入到一个作业队列中了： 12./rsync.sh &amp;jobs 对于已经在前台执行的命令，也可以重新放到后台执行，首先按ctrl+z暂停已经运行的进程，然后使用bg命令将停止的作业放到后台运行：bg %1，放回前台运行：%1。 但是如上方到后台执行的进程，其父进程还是当前终端shell的进程，而一旦父进程退出，则会发送hangup信号给所有子进程，子进程收到hangup以后也会退出。如果我们要在退出shell的时候继续运行进程，则需要使用nohup忽略hangup信号，或者setsid将将父进程设为init进程(进程号为1)：1234nohup ./rsync.sh &amp;setsid ./rsync.sh &amp;(./rsync.sh &amp;) # 在一个subshell中执行ps -ef|grep rsync nohup 的用途就是让提交的命令忽略 hangup 信号，标准输出和标准错误缺省会被重定向到 nohup.out 文件中。。一般我们可在结尾加上”&amp;”来将命令同时放入后台运行，也可用” &gt; log.out 2&gt;&amp;1”来更改缺省的重定向文件名。 上面的试验演示了使用nohup/setsid加上&amp;使进程在后台运行，同时不受当前shell退出的影响。那么对于已经在后台运行的进程，该怎么办呢？可以使用disown命令：123jobsdisown -h %1ps -ef|grep rsync 效果与setid相同，但是disown后无法通过jobs命令查看了。 Screen还有一种更加强大的方式是使用screen，首先创建一个断开模式的虚拟终端，然后用-r选项重新连接这个虚拟终端，在其中执行的任何命令，都能达到nohup的效果，这在有多个命令需要在后台连续执行的时候比较方便。 GNU Screen是一款由GNU计划开发的用于命令行终端切换的自由软件。用户可以通过该软件同时连接多个本地或远程的命令行会话，并在其间自由切换，可以看作是窗口管理器的命令行界面版本。它提供了统一的管理多个会话的界面和相应的功能。 123456789apt install screen # installscreen -S docker-d # 新建一个名叫docker-d的session，并马上进入screen -dmS docker-d # 新建一个名叫docker-d的session，但暂不进入，可用于系统启动脚本里screen -ls # 列出当前所有sessionscreen -r docker-d # 恢复到zhouxiao这个session，前提是已经是断开状态（-d可以远程断开会话）screen -x docker-d # 连接到离线模式的会话（多窗口同步演示）screen ./rsync.sh # screen创建一个执行脚本的单窗口会话，可以attach进程IDscreen -wipe # 检查目前所有的screen作业，并删除已经无法使用的screen作业 正常情况下，当你退出一个窗口中最后一个程序（通常是bash）后，这个窗口就关闭了。另一个关闭窗口的方法是使用C-a k，这个快捷键杀死当前的窗口，同时也将杀死这个窗口中正在运行的进程。 在每个screen session 下，所有命令都以 ctrl+a(C-a) 开始。123Ctrl-a w # 显示所有窗口列表Ctrl-a k # 这个快捷键杀死当前的窗口，同时也将杀死这个窗口中正在运行的进程。 Ctrl-a d # detach，暂时离开当前session 需要了解的是，一个用户创建的screen，其他用户（甚至root）通过screen -ls是看不见的。另外，Ctrl+a在bash下是用来回到行开头，不幸与上面的组合快捷键冲突。","tags":[{"name":"linux","slug":"linux","permalink":"http://starsci.cn/tags/linux/"}]},{"title":"恢复windows下右键的从vs code中打开工程","date":"2017-02-28T07:36:02.000Z","path":"2017/02/28/tools/resume-open-in-vs-code/","text":"本来vs code在安装的时候可以勾选上Add open with Code，可是不知道怎么突然消失了，可能某一天QQ管家作死把它弄掉了吧。vs code在文件夹中打开的功能非常好用，因此还是得找回来。具体方法如下： 1234567891011121314151617181920212223242526272829Windows Registry Editor Version 5.00; Open files[HKEY_CLASSES_ROOT\\*\\shell\\Open with VS Code]@=&quot;Edit with VS Code&quot;&quot;Icon&quot;=&quot;C:\\\\Program Files (x86)\\\\Microsoft VS Code\\\\Code.exe,0&quot;[HKEY_CLASSES_ROOT\\*\\shell\\Open with VS Code\\command]@=&quot;\\&quot;C:\\\\Program Files (x86)\\\\Microsoft VS Code\\\\Code.exe\\&quot; \\&quot;%1\\&quot;&quot;; This will make it appear when you right click ON a folder; The &quot;Icon&quot; line can be removed if you don&apos;t want the icon to appear[HKEY_CLASSES_ROOT\\Directory\\shell\\vscode]@=&quot;Open Folder as VS Code Project&quot;&quot;Icon&quot;=&quot;\\&quot;C:\\\\Program Files (x86)\\\\Microsoft VS Code\\\\Code.exe\\&quot;,0&quot;[HKEY_CLASSES_ROOT\\Directory\\shell\\vscode\\command]@=&quot;\\&quot;C:\\\\Program Files (x86)\\\\Microsoft VS Code\\\\Code.exe\\&quot; \\&quot;%1\\&quot;&quot;; This will make it appear when you right click INSIDE a folder; The &quot;Icon&quot; line can be removed if you don&apos;t want the icon to appear[HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\vscode]@=&quot;Open Folder as VS Code Project&quot;&quot;Icon&quot;=&quot;\\&quot;C:\\\\Program Files (x86)\\\\Microsoft VS Code\\\\Code.exe\\&quot;,0&quot;[HKEY_CLASSES_ROOT\\Directory\\Background\\shell\\vscode\\command]@=&quot;\\&quot;C:\\\\Program Files (x86)\\\\Microsoft VS Code\\\\Code.exe\\&quot; \\&quot;%V\\&quot;&quot; 保存成.reg文件，运行即可找回丢失的右键选项了。 this DaveJ— Right click on Windows folder and open with Visual Studio Code Marketing","tags":[{"name":"vscode","slug":"vscode","permalink":"http://starsci.cn/tags/vscode/"}]},{"title":"关于图像格式的那点事情","date":"2017-02-27T01:50:31.000Z","path":"2017/02/27/computer-vision/relate-img-precision/","text":"最近遇到一些有关Image图像格式的问题，精度确实在一定程度上很影响识别的精度。以前没有太过注意PIL库的使用方式，直到自己构建了一个tensorflow example之后才发现accuracy的不一致。这里仔细探究一下图像格式的一些问题。123456789101112131415161718from PIL import Imageimport numpy as npim_ori = Image.open('img_01.jpg')im_ori.save('img_01_save.jpg', quality=75)im = Image.open('img_01_save.jpg')im_ori_np = np.asarray(im_ori, dtype='float32')print('original pic:')print(im_ori_np[0][0:3])im_np = np.asarray(im, dtype='float32')print('\\nreload pic:')print(im_np[0][0:3])print('\\nMSE:')print(((im_ori_np-im_np)**2).mean(axis=None)) 首先是读入一张77,535 Byte的JPG格式的图像，然后再以默认的方式输出。结果发现输出后的图像大小为37,061 Byte也就是说，默认输出是存在有损压缩的。经查阅文档发现，有如下内容： The image quality, on a scale from 1 (worst) to 95 (best). The default is 75. Values above 95 should be avoided; 100 disables portions of the JPEG compression algorithm, and results in large files with hardly any gain in image quality. Image- save function 既然这样，那我们将quality增加到95，输出MSE，观察结果。发现MSE结果从55.8553降至2.50801，看起来似乎很小了，RGB三通道只有一两位有误差。但有没有零误差的方法呢？这里就不能使用JPG格式的图像了，必须要BMP格式。 The BMP file format is capable of storing two-dimensional digital images both monochrome and color, in various color depths, and optionally with data compression, alpha channels, and color profiles. Wiki- BMP Format 输出以im_ori.save(&#39;img_01_save.bmp)该方式，检查MSE结果为0，Good Job。","tags":[{"name":"python","slug":"python","permalink":"http://starsci.cn/tags/python/"},{"name":"image","slug":"image","permalink":"http://starsci.cn/tags/image/"},{"name":"format","slug":"format","permalink":"http://starsci.cn/tags/format/"}]},{"title":"git使用指南","date":"2017-02-25T00:48:28.000Z","path":"2017/02/25/tools/how-to-use-git/","text":"提交到远程服务器123456touch README.md # 新建说明文件git init # 在当前项目目录中生成本地git管理,并建立一个隐藏.git目录git add . # 添加当前目录中的所有文件到索引git commit -m \"first commit\" # 提交到本地源码库，并附加提交注释git remote add origin https://github.com/chape/test.git # 添加到远程项目，别名为origingit push -u origin master # 把本地源码库push到github 别名为origin的远程项目中，确认提交 从服务器初始化仓库1234touch README.md # 新建说明文件git initgit remote add origin https://github.com/chape/test.gitgit pull 修改文件并上传123git add .git commit -m \"update test\" # 检测文件改动并附加提交注释git push -u origin master # 提交修改到项目主线 删除历史提交中的大文件在不懂事的时候上传了一些.txt的大数据文件，最近突然发现git大了很多，经过反复核对，发现过去有一次提交了数据文件。因此，需要将那一次的提交的数据文件从git history中全部删除。12345678910111213141516171819# 查看当前git的容量 `size-pack`git count-objects -v# 显示最大的20个文件git verify-pack -v .git/objects/pack/pack-*.idx | sort -k 3 -n | tail -20 ## 查看具体该文件名git rev-list --objects --all | grep &#123;HASH&#125;# 从历史中删除该文件(git filter-branch --force --index-filter \\'git rm --cached --ignore-unmatch issue/*.txt' \\--prune-empty --tag-name-filter cat -- 1f243d1..# --1f243d1 指修改从该次提交后的所有历史 或者输入 -- --all 修改全部历史# 更新远程仓库git push --force# 删除历史引用等rm -Rf .git/refs/originalrm -Rf .git/logs/git gc# 查看当前仓库大小git count-objects -v 其它的一些123456789git push origin master # 把本地源码库push到Github上git pull origin master # 从Github上pull到本地源码库git config --list # 查看配置信息git status # 查看项目状态信息git branch # 查看项目分支git checkout -b host# 添加一个名为host的分支git checkout master # 切换到主干git merge host # 合并分支host到主干git branch -d host # 删除分支host","tags":[{"name":"git","slug":"git","permalink":"http://starsci.cn/tags/git/"},{"name":"github","slug":"github","permalink":"http://starsci.cn/tags/github/"}]},{"title":"安装tensorflow r1.0的各种坑","date":"2017-02-24T05:11:31.000Z","path":"2017/02/24/deep-learning/install-tensorflow-helper/","text":"Install on Win10我的笔记本是GTX960M，查了一下cuda capacity = 5.0，还可以支持CUDA8.0和cuDNN5.1这就是传说中的引言。记得一段时间以前还不支持windows，现在tensorflow也支持windows platform了。很早就听说它的架构设计的非常好，也一直想学习这样的模式。所以刚好有一些许时间，就打算看看它的模式。当然，还是习惯c++，因此又在虚拟机下安装了tf，由于vmware不知道怎么的安不上ubuntu16.04，只好换了virtualbox，这够折腾。此外，记得准保好vpn工具，不然有些资源可能会下载不下来。 windows 安装windows下貌似只支持python的安装。一直以来我用的是anaconda2-python27，结果告诉只有35版本。后来没注意下了Anaconda3-python36，结果依旧不可以用。好吧，反正Anaconda也用的不多，就干脆用原生的python35+vs code好了。 首先下载并安装CUDA8.0，cuDNN5.2 并加入path路径12C:\\extern\\cudnn5.1\\binC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64` # important! 安装tesorflow pip3 install --upgrade tensorflow-gpu Visual studio code 设置推荐使用python插件，自带pylint规范化代码，挺有意思的。此外，在文件夹中打开，并配置一个tasks.json。可以使用ctrl+shift+B快速运行。我的配置代码如下：12345678910 &#123; // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format \"version\": \"0.1.0\", \"command\": \"python\", \"owner\": \"py\", \"isShellCommand\": true, \"args\": [\"$&#123;file&#125;\"], \"showOutput\": \"always\"&#125; 输入tensorflow官方测试代码：1234567import tensorflow as tfnode1 = tf.constant(3.0, tf.float32)node2 = tf.constant(4.0) # also tf.float32 implicitlyprint(node1, node2)sess = tf.Session()print(sess.run([node1, node2])) 应该有正常输出的结果，以及一大串debug信息。包括设备(GPU)CUDA的连接情况以及设备型号等等。如果不想输出类似的结果，可以在开头import tensorflow as tf之前，插入:12import osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 就最小化输出了。 Ubuntu 16.04 安装ubuntu16.04的python安装就不说了，蛮简单的，和windows类似，这里说几个可能会用的指令。Q：很多包(.sh之类)基于用户身份安装，但有些关联的包需要用sudo，却找不到用户安装的包。例如用户安装了python，但在sudo python却找不到命令。A：将路径添加进用户变量：编辑.bashrc,最后添加alias sudo=&#39;sudo env PATH=$PATH&#39;, 然后执行source ~/.bashrc 安装bazel, 在官方网站下载下最新的bazel.sh注意安装bazel之前，首先需要安装jre和jdk，在ubuntu16.04直接使用apt安装就好1234sudo apt install default-jresudo apt install default-jdkchmod +x bazel*.sh./bazel*.sh记得最后它要求把环境变量加进去的时候自己打进去。此外再加入一行。123source /home/yourname/.bazel/bin/bazel-complete.bashexport PATH=$PATH:/home/yourname/.bazel/binsource ~/.bashrc 安装tensorflow，从github上clone下来后1234567891011121314sudo apt-get install libcurl3 libcurl3-devsudo apt-get install zlib1g-devsudo ./configure# bazel安装bazel build --config opt //tensorflow/tools/pip_package:build_pip_packagemkdir _python_buildcd _python_buildln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/* .ln -s ../tensorflow/tools/pip_package/* .sudo python setup.py develop# run train samplegit clone https://github.com/tensorflow/modelscd models/tutorials/image/mnistpython convolutional.py 更多请参考该网站。 Google— Install Tensorflow","tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://starsci.cn/tags/tensorflow/"}]},{"title":"如何使用hexo写blog","date":"2017-02-23T09:05:31.000Z","path":"2017/02/23/tools/how-to-use-hexo/","text":"一般的引言：这就是传说中的引言。没有作者的出处。 出自某本书：Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy. David Levithan— Wide Awake 出自网络上的某篇文章：Every interaction is both precious and an opportunity to delight. Seth Godin— Welcome to Island Marketing 代码区域：1234#incldue &lt;iostream&gt;int main() &#123; std::cout &lt;&lt; \"hello world\" &lt;&lt; std::endl;&#125; 插入图片需要使用：hexo new [layout] &lt;title&gt; 公式注意特殊字符需要转义下划线_需要转义为\\_$$ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})^2 $$ iframe-比如外链播放器","tags":[{"name":"hexo","slug":"hexo","permalink":"http://starsci.cn/tags/hexo/"}]}]
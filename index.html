<!DOCTYPE html>





<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="Life is Wonderful!">
<meta property="og:type" content="website">
<meta property="og:title" content="THE ISLET">
<meta property="og:url" content="http://kaijin.ai/index.html">
<meta property="og:site_name" content="THE ISLET">
<meta property="og:description" content="Life is Wonderful!">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="THE ISLET">
<meta name="twitter:description" content="Life is Wonderful!">
  <link rel="canonical" href="http://kaijin.ai/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>THE ISLET</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">
  <div class="container">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">THE ISLET</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Amor Mundi</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-schedule">
      
    

    <a href="/schedule/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>Schedule</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-home">
            
  <section id="posts" class="posts-expand">
      

  <article class="post" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://kaijin.ai/2017/12/23/deep-learning/loss-view/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kai JIN">
      <meta itemprop="description" content="Life is Wonderful!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="THE ISLET">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2017/12/23/deep-learning/loss-view/" class="post-title-link" itemprop="url">损失函数概览</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2017-12-23 11:48:31" itemprop="dateCreated datePublished" datetime="2017-12-23T11:48:31+08:00">2017-12-23</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-08-22 16:37:12" itemprop="dateModified" datetime="2019-08-22T16:37:12+08:00">2019-08-22</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="kl散度和交叉熵的区别">KL散度和交叉熵的区别</h3>
<p>在上一篇关于 <code>GAN</code> 理论的文章中提到了 <code>KL</code> 散度以及说它也可以称为交叉熵。但实际上，交叉熵与它有一些微小的差异。我们先回顾一下 <code>KL</code> 散度的公式：<span class="math display">\[D(p\|q)=\sum\limits_{i=1}^{n}p(x)\log\frac{p(x)}{q(x)} = \sum\limits_{i=1}^{n}p(x)\log p(x) - \sum\limits_{i=1}^{n}p(x)\log q(x)\]</span> 我们注意到，其实前半部分在实际中相当于一个常数项，在给定 <span class="math inline">\(p\)</span> 分布的情况下。后半部分我们称为交叉熵，它衡量了分布 <span class="math inline">\(p\)</span> 和 <span class="math inline">\(q\)</span> 的距离。然而，在深度学习优化过程中，常数项不影响优化的结果，所以我们往往采用简化的形式。</p>
<p>正式的说，交叉熵(cross entropy)定义为：<span class="math display">\[H(p,q)=-\int p(x)\log q(x)dx \]</span> 其中，<span class="math inline">\(p(x\)</span>) 是数据的真实分布的概率，<span class="math inline">\(q(x)\)</span> 是由数据计算得到的概率分布。我们期望计算得到的分布尽可能的接近真实分布。下面，我们将正式的介绍损失函数，它的变化形式，以及在DNN中的来源，影响，和作用。</p>
<h3 id="损失函数">损失函数</h3>
<p>在以前的一篇文章中介绍了反向传播算法的公式推导，然而Loss Function往往位于网络的顶层。因此，我们在这里只考虑顶层设计以及前一层的前向传播和方向传播。假设网络最后一层 <span class="math inline">\(l\)</span> 的线性输出为 <span class="math inline">\(a^{(l+1)}=W^{(l)}x^{(l)}+b^{(l)}\)</span>，非线性映射函数为 <span class="math inline">\(\widetilde{y}=f(a^{(l+1)})\)</span>。Loss Function是衡量预测分布与真实分布的一种度量手段，这里我们用 <span class="math inline">\(L=D(y, \widetilde{y})\)</span> 表示与目标分布的距离。在反向传播阶段，我们需要求出前层梯度值： <span class="math display">\[
\begin{align}
\nabla_{W^{(l)}}L=\frac{\partial L}{\partial a^{(l+1)}}\frac{\partial a^{(l+1)}}{\partial W^{(l)}}=\delta^{(l+1)}x^{(l)} \nonumber \\
\nabla_{b^{(l)}}L=\frac{\partial L}{\partial a^{(l+1)}}\frac{\partial a^{(l+1)}}{\partial b^{(l)}}=\delta^{(l+1)}
\end{align}
\]</span></p>
<h3 id="l2-loss">L2 Loss</h3>
<p>现在，我们考虑一种传统的损失函数，比如 <code>l2-loss</code>，其距离分布定义为：<span class="math display">\[D_{l2}(y, \widetilde{y})=\\|y-\widetilde{y}\\|_2^2\]</span> 在这里 <span class="math inline">\(y\)</span> 和 <span class="math inline">\(\widetilde{y}\)</span> 可以是一个标量也可以是一个向量。将该损失函数代入至反向传播过程中，易得： <span class="math display">\[
\begin{align} 
\delta^{(l+1)} &amp;= \frac{\partial D_{l2}}{\partial a^{(l+1)}} = \frac{\partial D_{l2}}{\partial \widetilde{y}}\frac{\partial \widetilde{y}}{\partial a^{(l+1)}} \nonumber \\
&amp;= -2(y-\widetilde{y})f&#39;(a^{(l+1)})
\end{align}
\]</span></p>
<p>如果考虑网络的目标输出采用 <code>sigmoid</code> 激活函数，<span class="math inline">\(a^{(l+1)}\)</span> 简化为 <span class="math inline">\(x\)</span>，即：<span class="math display">\[ \widetilde{y} = {\rm sigmoid}(x) = \frac{1}{1+e^{-x}} \]</span> 那么其微分形式为：<span class="math display">\[f&#39;(x)= \frac{e^{-x}}{(1+e^{-x})^2}=f(x)(1-f(x)) \]</span> 将其代入至反向传播过程中：<span class="math display">\[ \delta^{(l+1)} = -2(y-f(a^{(l+1)}))f(a^{(l+1)})(1-f(a^{(l+1)})) \]</span></p>
那么，对于每一个线性输出，最终梯度的更新量是多少呢？在这里，我们假定一个二分类问题，即 <span class="math inline">\(y\in{1, 0}\)</span>，结果如下图所示。
<center>
<img src="/image/loss-function-l2-weight.png" width="100%">
</center>
<p>这是一个很有趣的结果，我们可以看到 <span class="math inline">\(y=1, y=0\)</span> 在 <span class="math inline">\(W\)</span> 的梯度值是关于 <span class="math inline">\(y\)</span> 轴对称。也就是说对任意的 <span class="math inline">\(x\)</span>，接近 <span class="math inline">\(1\)</span> 即远离 <span class="math inline">\(0\)</span>，对于前者更新值为正，对于后者为负，非常的reasonable。这里，我看到网上有些博客说因为 <code>l2-loss</code> 会导致收敛变慢，所以采用交叉熵作为损失函数，我觉得是有一些问题的，在这里持保留态度。</p>
<h3 id="cross-entropy-loss">Cross-Entropy Loss</h3>
<p>刚刚提到了 <code>l2-loss</code> ，我们从其推导出交叉熵损失函数的公式。考虑梯度的一般形式： <span class="math display">\[\delta^{(l+1)}=\frac{\partial L}{\partial \widetilde{y}}\widetilde{y}(1-\widetilde{y})\]</span> 在 <code>l2-loss</code> 的形式下，<span class="math display">\[\delta^{(l+1)}=-2(y-\widetilde{y})\widetilde{y}(1-\widetilde{y})\]</span> 那么我们是否有可能通过某种手段，消去后面的 <span class="math inline">\(\widetilde{y}(1-\widetilde{y})\)</span>。也就是说，我们希望构造一个 <span class="math inline">\(D_e\)</span>，它满足：<span class="math inline">\(\delta^{(l+1)} = -2(y-\widetilde{y})\)</span>。这样，代入到梯度的一般形式中，可得：<span class="math display">\[ \frac{\partial D_{e}}{\partial \widetilde{y}}\widetilde{y}(1-\widetilde{y}) = -2(y-\widetilde{y}) \]</span></p>
<p>下面，我们需要解 <span class="math inline">\(D_e\)</span>，相当于一个不定积分。证明如下： <span class="math display">\[
\begin{align}
D_e &amp;= -2\int\frac{y-\widetilde{y}}{\widetilde{y}(1-\widetilde{y})}d\widetilde{y} \nonumber \\
&amp; = -2\int[\frac{y}{\widetilde{y}}+\frac{y-1}{1-\widetilde{y}}]d\widetilde{y} \nonumber \\
&amp; = -2[\int yd\log\widetilde{y}+\int(1-y)d\log(1-\widetilde{y})] \nonumber \\
&amp; = -2[y\log\widetilde{y}+(1-y)\log(1-\widetilde{y})] + C \nonumber \\
&amp; \approx -y\log\widetilde{y}-(1-y)\log(1-\widetilde{y})
\end{align}
\]</span></p>
<p>注意，以上结果都是在激活函数为sigmoid的情况下推导而出。我们将线性输出值 $= $ 代入： <span class="math display">\[
\begin{align}
D_e &amp;= -y\log\widetilde{y}-(1-y)\log(1-\widetilde{y}) \nonumber \\
&amp;= -y\log\frac{1}{1+e^{-a}} - (1-y)\log(1-\frac{1}{1+e^{-a}})  \nonumber \\
&amp;= \log(1+e^{-a})+a(1-y)
\end{align}
\]</span></p>
<p>上式便是在tensorflow中的简化求解形式。尽管观察直接的更新率，由 <span class="math inline">\(-2(y-\widetilde{y})\widetilde{y}(1-\widetilde{y})\)</span> 变成了 <span class="math inline">\(-(y-\widetilde{y})\)</span> 增加了反向传播的梯度值。</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://kaijin.ai/2017/12/22/deep-learning/gan-basic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kai JIN">
      <meta itemprop="description" content="Life is Wonderful!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="THE ISLET">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2017/12/22/deep-learning/gan-basic/" class="post-title-link" itemprop="url">GAN的理论基础</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2017-12-22 09:50:31" itemprop="dateCreated datePublished" datetime="2017-12-22T09:50:31+08:00">2017-12-22</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-08-22 16:37:12" itemprop="dateModified" datetime="2019-08-22T16:37:12+08:00">2019-08-22</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="最大似然估计-maximum-likelihood-estimation">最大似然估计 Maximum Likelihood Estimation</h3>
<p>考虑一个给定的数据分布 <span class="math inline">\(P_{d}(x)\)</span> 和一个已知的分布 <span class="math inline">\(P_g(x;\theta)\)</span> 由 <span class="math inline">\(\theta\)</span> 参数化。我们希望优化 <span class="math inline">\(\theta\)</span> 使得 <span class="math inline">\(P_g(x;\theta)\)</span> 接近 <span class="math inline">\(P_{d}(x)\)</span>。然后，我们从 <span class="math inline">\(P_{d}(x)\)</span> 采样 <span class="math inline">\(\{x^1, x^2, \dots, x^m\}\)</span>，生成样本的似然度定义为: <span class="math display">\[ L = \prod\limits_{i=1}^{m}P_g(x;\theta) \]</span> 找到一个 <span class="math inline">\(\theta^*\)</span> 最大化这个似然估计: <span class="math display">\[
\begin{align}
\theta^*&amp;=\arg\max_\theta\prod\limits_{i=1}^{m}P_g(x^i;\theta)=\arg\max_\theta\log\prod\limits_{i=1}^{m}P_g(x^i;\theta) \nonumber \\
&amp;=\arg\max_{\theta}\sum\limits_{i=1}^{m}\log P_g(x^i;\theta), x \sim P_{d}(x) \nonumber \\
&amp;\approx\arg\max_\theta\mathbb{E}_{x\sim P_{d}}[\log P_g(x;\theta)]
\end{align}
\]</span></p>
<p>这里, 为进一步说明，我们需要引入一个概念 <code>KL散度</code>, 或 <code>Kullback–Leibler divergence</code>.</p>
<h3 id="kl-散度">KL 散度</h3>
<p>KL散度又称为相对熵，首先信息熵反映了一个系统的有序程度，一个系统越是有序，它的信息熵就越低，反之越高。如果一个随机变量 <span class="math inline">\(X\)</span> 的可能取值为 <span class="math inline">\(X=\{x_1,x_2,\dots,x_n\}\)</span>，对应的概率为 <span class="math inline">\(p(X=x_i) (i=1,2,\dots,n)\)</span>，则随机变量 <span class="math inline">\(X\)</span> 的熵定义为: <span class="math display">\[ H(X)=-\sum\limits_{i=1}^{n}p(x_i)\log p(x_i) \]</span></p>
<p>相对熵又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度等。设 <span class="math inline">\(p(x)\)</span> 和 <span class="math inline">\(q(x)\)</span> 是 <span class="math inline">\(X\)</span> 取值的两个概率分布，则 <span class="math inline">\(p\)</span> 对 <span class="math inline">\(q\)</span> 的相对熵为：<span class="math display">\[D(p\|q)=\sum\limits_{i=1}^{n}p(x)\log\frac{p(x)}{q(x)} = \sum\limits_{i=1}^{n}p(x)\log p(x) - \sum\limits_{i=1}^{n}p(x)\log q(x)\]</span> 在一定程度上，熵可以度量两个随机变量的距离。KL散度是两个概率分布 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(Q\)</span> 差别的非对称性的度量。KL散度是用来度量使用基于 <span class="math inline">\(Q\)</span> 的编码来编码来自 <span class="math inline">\(P\)</span> 的样本平均所需的额外的位元数。典型情况下，<span class="math inline">\(P\)</span> 表示数据的真实分布，<span class="math inline">\(Q\)</span> 表示数据的理论分布，模型分布，或 <span class="math inline">\(P\)</span> 的近似分布。</p>
<p>好的，回归GAN的主题。刚刚我们已经推导出最大似然估计下的参数 <span class="math inline">\(\theta^*\)</span> 的表示。然而，在网络中，我们需要最小化去优化这个参数。因此，我们需要利用<code>KL散度</code>完成这件事情。<span class="math display">\[ \theta^*\approx\arg\max_\theta\mathbb{E}_{x\sim P_{d}}[\log P_g(x;\theta)] = \arg\max_\theta\int_x P_d(x)\log P_g(x;\theta)dx \]</span></p>
<p>上式恰好表示的是从 <span class="math inline">\(d\)</span> 中采样的样本基于 <span class="math inline">\(G\)</span> 的编码的度量。为凑成 <code>KL散度</code> 的形式，我们增加一项 $ _{xP_d}[P_d(x;)] <span class="math inline">\(。考虑到其为一个定值，\)</span>^*$的优化目标可以转化为： <span class="math display">\[
\begin{align}
\theta^*&amp;\approx\arg\max_\theta(\mathbb{E}_{x\sim P_{d}}[\log P_g(x;\theta)] - \mathbb{E}_{x\sim P_d}[\log P_d(x;\theta)]) \nonumber \\
&amp;=\arg\max_\theta(\int_x P_{d}(x)\log P_g(x;\theta)dx-\int_x P_{d}(x)\log P_{d}(x)dx) \nonumber \\
&amp;=\arg\min_\theta(\int_x P_{d}(x)\log P_{d}(x)dx-\int_x P_{d}(x)\log P_g(x;\theta)dx) \nonumber \\
&amp;=\arg\min_\theta \rm{KL}(P_{d}(x)\|P_g(x;\theta))
\end{align}
\]</span></p>
<h3 id="basic-idea-of-gan">Basic idea of GAN</h3>
<p>既然说我们是很难优化 <span class="math inline">\(P_g(x;\theta)\)</span> 这个分布 (因为很难找到这样一个函数)。那么GAN是如何解决这个问题的呢？ - <code>Generator</code>: G 是一个函数，输入<span class="math inline">\(z\)</span>，输出<span class="math inline">\(x\)</span>。给定一个先验分布 <span class="math inline">\(P_{prior}(z)\)</span>，G 定义了一个概率分布 <span class="math inline">\(P_g(x)\)</span> - <code>Discrimnator</code>: 衡量 <span class="math inline">\(P_g(x)\)</span> 与 <span class="math inline">\(P_d(x)\)</span> 之间的差距大小。输入 <span class="math inline">\(x\)</span>，输出一个标量。</p>
我们定义一个函数：<span class="math display">\[ G^*=\arg\min\limits_G\max\limits_DV(G, D) \]</span> 上面的式子该怎么理解呢？如下图。我们首先先选定一个 <span class="math inline">\(G_i\)</span> 对应一个 <span class="math inline">\(D\)</span> 的分布，<span class="math inline">\(\max\limits_D V\)</span> 从中选取该分布上值最大的点加入集合。最后，我们在所有 <span class="math inline">\(G_i\)</span> 的点中选取值最小的 <span class="math inline">\(G_i\)</span> 作目标输出。
<center>
<img src="/image/basic-gan-maxmin.png" width="70%">
</center>
<p>是不是还有点困惑，是的，我们具体定义下式： <span class="math display">\[ V=\mathbb{E}_{x\sim P_{d}}[\log D(x)] + \mathbb{E}_{x\sim P_g}[\log(1-D(x))] \]</span> 给定一个<code>G</code>，<span class="math inline">\(\max\limits_D V(G,D)\)</span>表示的是 <span class="math inline">\(P_g\)</span> 和 <span class="math inline">\(P_d\)</span>的差值。这样对于每一个 <span class="math inline">\(G_i\)</span>相当于与目标分布的最大差值，在这一系列差值中找到最小的，也就是说最好的那个 <span class="math inline">\(G_i\)</span>。换言之，在所有最差(差值最大的情况下)的情况下找一个最好的。</p>
<p>既然我们已经有了这样一个定义，那么给定一个 <span class="math inline">\(V\)</span> 和 <span class="math inline">\(G_i\)</span> 我们该如何寻找一个最大的 <span class="math inline">\(D^*\)</span> 呢？ <span class="math display">\[
\begin{align}
V&amp;=\mathbb{E}_{x\sim P_{d}}[\log D(x)] + \mathbb{E}_{x\sim P_g}[\log(1-D(x))] \nonumber \\
&amp;=\int_x P_d(x)\log D(x)dx + \int_x P_g(x)\log(1-D(x))dx \nonumber \\
&amp;=\int_x [P_d(x)\log D(x) +P_g(x)\log(1-D(x))]dx
\end{align}
\]</span></p>
<p>给定一个 <span class="math inline">\(x\)</span> 的情况下，为使上式积分内部项最大，<span class="math inline">\(D^*\)</span> 应该是多少呢？首先，<span class="math inline">\(P_d(x)\)</span> 是给定的，<span class="math inline">\(P_g(x)\)</span> 也是给定的(在某个<span class="math inline">\(G_i\)</span>的情况下)，那么上式将转变为： <span class="math display">\[
\begin{align}
f(D) &amp;= P_d\log(D) + P_g\log(1-D) \nonumber \\
f&#39;(D) &amp;= \frac{P_d}{D} - \frac{P_g}{1-D} = 0 \nonumber \\
D^*(x) &amp;= \frac{P_d(x)}{P_d(x)+P_g(x)}
\end{align}
\]</span></p>
<p>我们所求得的 <span class="math inline">\(D^*\)</span> 便是上文所说的最大差值(maximum divergence)，将其带入 <span class="math inline">\(V\)</span> 中： <span class="math display">\[
\begin{align}
V &amp;= \mathbb{E}_{x\sim P_d}[\log\frac{P_d(x)}{P_d(x)+P_g(x)}] + \mathbb{E}_{x\sim P_g}[\log\frac{P_g(x)}{P_d(x)+P_g(x)}] \nonumber \\
&amp;= \int_x P_d(x)\log\frac{\frac{1}{2}P_d(x)}{\frac{1}{2}(P_d(x)+P_g(x))}dx + \int_x P_g(x)\log\frac{\frac{1}{2}P_g(x)}{\frac{1}{2}(P_d(x)+P_g(x))}dx \nonumber \\
&amp;= -2\log2+\int_x P_d(x)\log\frac{P_d(x)}{\frac{1}{2}(P_d(x)+P_g(x))}dx + \int_x P_g(x)\log\frac{P_g(x)}{\frac{1}{2}(P_d(x)+P_g(x))}dx \nonumber \\
&amp;= -2\log2 + \rm{KL}[P_d(x)\|\frac{1}{2}(P_d(x)+P_g(x))] + \rm{KL}[P_g(x)\|\frac{1}{2}(P_d(x)+P_g(x))] 
\end{align}
\]</span></p>
<p>这个结果很有意思，上式最后两项互相对称。换言之，它衡量了两个分布之间的距离（无关顺序）。此外，这个Divergence叫做 <code>Jensen-Shanno divergence</code> 简称 <code>JSD</code>，定义如下：<span class="math display">\[ \rm{JSD}(P\|Q) = \frac{1}{2}D(P\|\frac{1}{2}(P+Q)) + \frac{1}{2}D(Q\|\frac{1}{2}(P+Q)) \]</span></p>
<p>因此，原始的GAN实际上的目标是一个JSD。因此，对于给定的 <span class="math inline">\(G_i\)</span>，最大化 <span class="math inline">\(D\)</span> 的结果为： <span class="math display">\[\max\limits_D V(G, D) = -2\log2 + 2\rm{JSD}(P_d(x)\|P_g(x))\]</span></p>
<p>JSD的值域在 <span class="math inline">\((0, \log2)\)</span>，当 <span class="math inline">\(P_g(x)=P_d(x)\)</span> 时，JSD的值为 <span class="math inline">\(0\)</span>；反之两个分布完全不相干时，JSD的值为 <span class="math inline">\(log2\)</span>。因此，要找到最小的 <span class="math inline">\(G_i\)</span>，那么我们就需要使 <span class="math inline">\(G_i\)</span> 尽可能接近 <span class="math inline">\(P_d(x)\)</span>。</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://kaijin.ai/2017/10/21/deep-learning/back-propagation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kai JIN">
      <meta itemprop="description" content="Life is Wonderful!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="THE ISLET">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2017/10/21/deep-learning/back-propagation/" class="post-title-link" itemprop="url">深度学习中的反向传播算法</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2017-10-21 00:44:32" itemprop="dateCreated datePublished" datetime="2017-10-21T00:44:32+08:00">2017-10-21</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-08-22 16:37:12" itemprop="dateModified" datetime="2019-08-22T16:37:12+08:00">2019-08-22</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>反向传播算法直观上理解似乎很容易，但具体到每一个细节，每一个层面就变得很奇怪了。一直都没有正视这个问题，今天打算认真推导一遍这些公式。</p>
<h3 id="back-propagation">Back-Propagation</h3>
<p>考虑深度网络中的任意一层，其中 <span class="math inline">\(a^{(l)}\)</span> 表示第 <span class="math inline">\(l\)</span> 层的输入数据，<span class="math inline">\(a^{(l+1)}\)</span> 表示该层的输出，同时也是 <span class="math inline">\((l+1)\)</span> 层的输入数据： - 线性映射输出：$ z<sup>{(l+1)}=W</sup>{(l)}a<sup>{(l)}+b</sup>{(l)}$ - 非线性映射输出：$ a^{(l+1)} = f(z^{(l+1)})$</p>
<p>如果 <span class="math inline">\(J(W,b)\)</span> 是顶层的损失函数，那么我们想直到该损失对神经网络的每一层的影响是怎样的？只需要求出该损失值相应 <span class="math inline">\(W\)</span> 和 <span class="math inline">\(b\)</span> 的变化即可。根据链式求导法则，如果我们想要求损失值对第 <span class="math inline">\(l\)</span> 层的 <span class="math inline">\(W^{(l)}\)</span> 的影响，可以中间增加 $z^{(l)} 项。<strong>注意：这一项的选取恰好是线性变换的输出。</strong></p>
<p><span class="math display">\[ \nabla_{W^{(l)}}J(W,b) = \frac{\partial J(W,b)}{\partial z^{(l+1)}} \frac{\partial z^{(l+1)}}{\partial W^{(l)}} = \delta^{(l+1)}(a^{(l)})^T \]</span></p>
<p><span class="math display">\[ \nabla_{b^{(l)}}J(W,b) = \frac{\partial J(W,b)}{\partial z^{(l+1)}} \frac{\partial z^{(l+1)}}{\partial b^{(l)}} = \delta^{(l+1)} \]</span></p>
<p>那么现在的问题是如何求我们定义的新符号 <span class="math inline">\(\frac{\partial J(W,b)}{\partial z^{(l+1)}} = \delta^{(l+1)}\)</span> 呢？事实上，在这里，我们需要找到的其实是 <span class="math inline">\(\delta^{(l)}\)</span> 与 <span class="math inline">\(\delta^{(l+1)}\)</span> 的一个<strong>关系</strong>。只要找到这个关系，我们通过初始条件，像递推数列一样依次推导而出。 <span class="math display">\[
\begin{align}
  \delta^{(l)} &amp;= \frac{\partial J(W,b)}{\partial z^{(l)}} = \frac{\partial J(W,b)}{\partial z^{(l+1)}} \frac{\partial z^{(l+1)}}{\partial z^{(l)}} \nonumber \\
   &amp;= \delta^{(l+1)} \frac{\partial (W^{(l)}a^{(l)}+b^{(l)})}{\partial z^{(l)}} \nonumber \\
   &amp;= \delta^{(l+1)} \frac{\partial (W^{(l)}f(z^{(l)})+b^{(l)})}{\partial z^{(l)}} \nonumber \\
   &amp;= \delta^{(l+1)} W^{(l)}f&#39;(z^{(l)})
\end{align}
\]</span></p>
<p>通常而言，我们在顶层的<code>全连接层</code>后不增加激活函数。因此，顶层的输出 $z^{(top)} 直接进入<code>softmax-loss</code>层(如果是回归问题)计算<code>损失值</code>。</p>
<p>举个例子来说。假设网络共有 <span class="math inline">\(n\)</span> 层，如果顶层返回的 <span class="math inline">\(loss=3.27\)</span> ，第 <span class="math inline">\(n\)</span> 层(<code>全连接层</code>)的输出为4.2(某个维度)。那么<span class="math inline">\(\delta^{(n)}=\frac{3.27}{4.2}=0.7786\)</span>。后面，就按照公式依次往下推即可。</p>
<h3 id="梯度消失问题">梯度消失问题</h3>
<p>如果我们采用<code>SGD</code>方法优化模型，那么权重的更新方法为： <span class="math display">\[ W^{(n+1)} = W^{(n)} - \epsilon\nabla J^{(n)} \]</span></p>
<p>梯度消失是在网络反向传播过程中，从顶层向顶层传播的值越来越小，以致于无限趋近于零，这将导致底层的网络无法学习到有效的内容。</p>
<p>我们考虑网络存在第 <span class="math inline">\(l\)</span> 层 和 <span class="math inline">\(l+k\)</span> 层，考虑上述公式：$ ^{(l)}= ^{(l+1)} W<sup>{(l)}f'(z</sup>{(l)}) $</p>
<p>那么，从 <span class="math inline">\(l\)</span> 项递推到 <span class="math inline">\(l+k\)</span> 项： <span class="math display">\[
\begin{align} 
  \delta^{(l)} &amp;= \delta^{(l+1)} W^{(l)}f&#39;(z^{(l)}) \nonumber \\
  &amp;= \delta^{(l+2)} (W^{(l+1)}f&#39;(z^{(l+1)})) (W^{(l)}f&#39;(z^{(l)})) \nonumber \\
  &amp;= \delta^{(l+k)} \prod_{m=l}^{l+k-1}W^{(m)}f&#39;(z^{(m)})
\end{align}
\]</span></p>
<p>注意，在这一步，我们推导出了第 <span class="math inline">\(l+k\)</span> 层与 <span class="math inline">\(l\)</span> 层属于连乘关系。那么如果我们的激活函数是<code>sigmoid：</code>$ S(x) = $ ，情况会变得怎样呢? <span class="math display">\[ S&#39;(x) = \frac{\mathrm{e}^{-x}}{(1+\mathrm{e}^{-x})^2} = S(x)(1-S(x)) \]</span></p>
<p>我们需要知道对于 <span class="math inline">\(S(x)\)</span> 函数的值域在<span class="math inline">\((-1, 1)\)</span>。因此，<span class="math inline">\(S&#39;(x)\)</span> 的第一项 <span class="math inline">\(S(x)\)</span> 在递推公式中经过连乘会不断变小，最终趋于零。</p>
<h3 id="resnet如何解决梯度消失">ResNet如何解决梯度消失</h3>
<p>ResNet是有主干网络和分支网络。主干网络路线中gate仅仅包含5个，而每一个block包含若干个residual unit。我们通常认为Resnet解决了深度的问题，而事实上它是另一种ensemble模型，改变了深度网络的优化模式，并没有从本质上解决梯度消失的问题（由反向传播的连乘本质决定的）。那么Resnet的残差模式为什么能够起作用呢？</p>
<p>考虑 <span class="math inline">\(F(x^{(l)}, W^{(l)})\)</span> 为第 <span class="math inline">\(l\)</span> 层的残差输出，其中 <span class="math inline">\(x^{(l)}\)</span> 为第 <span class="math inline">\(l\)</span> 层的输入，<span class="math inline">\(W^{(l)}\)</span> 是残差单元中一系列变换的集合。那么，残差单元的激活公式可由下式表示： - 第 <span class="math inline">\(l\)</span> 层的输出： $ y^{(l)} = x^{(l)} + F(x^{(l)}, W^{(l)}) $ - 映射关系： $ x^{(l+1)} = f(y^{(l)}) $</p>
<p>如果采用<code>identity map</code>方式进行映射，那么进行element-wise addition操作，即： <span class="math display">\[ x^{(l+1)} = x^{(l)} + F(x^{(l)}, W^{(l)}) \]</span></p>
<p>假设我们有 <span class="math inline">\(L\)</span> 个residual unit在一个block中，那么第 <span class="math inline">\(L\)</span> 层的输出可以用下式表示： <span class="math display">\[ x^{(L)} = x^{(l)} + \sum_{i=l}^{L-1}F(x^{(i)}, W^{(i)}) \]</span></p>
<p>对其中任意 <span class="math inline">\(l\)</span> 层求梯度，其中 <span class="math inline">\(\varepsilon\)</span> 是loss function： <span class="math display">\[ \frac{\partial \varepsilon}{\partial x^{(l)}} = \frac{\partial \varepsilon}{\partial x^{(L)}} \frac{\partial x^{(L)}}{\partial x^{(l)}} = \frac{\partial \varepsilon}{\partial x^{(L)}}\left(1+\frac{\partial}{\partial x^{(l)}}\sum_{i=1}^{L-1}F(x^{(i)}, W^{(i)}) \right) \]</span></p>
<p>因此，我们可以看到因为有 $ $ 这一项的存在，任意 <span class="math inline">\(l\)</span> 层总能接受到一定的信息。通过这种机制，我们可以充分利用浅层网络的优势从而训练深层网络。</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://kaijin.ai/2017/04/17/deep-learning/softmax-n-softmax-loss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kai JIN">
      <meta itemprop="description" content="Life is Wonderful!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="THE ISLET">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
            
            <a href="/2017/04/17/deep-learning/softmax-n-softmax-loss/" class="post-title-link" itemprop="url">Softmax vs. Sotmax-Loss 数值稳定性</a>
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2017-04-17 10:09:14" itemprop="dateCreated datePublished" datetime="2017-04-17T10:09:14+08:00">2017-04-17</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-08-22 16:37:12" itemprop="dateModified" datetime="2019-08-22T16:37:12+08:00">2019-08-22</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最近一直在和学校的老师做Deep Learning有关的课题，虽然说是在做学术上的事情，可是大多数仍然是工程方面的。现在Deep Learning的一个很大的趋势是工程化。虽然学术界有很多看起来很漂亮的理论和方法，但在实际使用中，用的往往不多，反倒是经过历史检验的那么几种简单朴素的方法却是用途最为广泛。这也是一件挺有意思的事情，越复杂的问题反倒需要越简单的想法去解决，比如<code>SGD</code>，<code>Dropout</code>，<code>BN</code>等等，都不是很复杂的模型，但却是用途最为广泛的技巧。另一个方面是，算法的变革所提升的精度远远不如数据集的扩增，网络的增大收益来的快和简单。因此，工业界往往喜欢boost网络的训练速度，增大数据集，然后不断的调参。</p>
<p>只是最近的一些感慨，我还是蛮喜欢数学上精巧的东西。有意思的事情，前两天一个初中小女孩问我数学题，本来以为信手捏来，结果愣生生把一道简单的集合题做成了解析几何，还需要求高次方程，汗颜。不要问我结果，我已经让那个孩子去问老师了，回来给我讲一讲。这两天把以前保存的文章拿出来，好好咀嚼一下。过去看到一篇好的文章总爱收藏起来却很久再也没有翻开过，似乎看了很多文章，实际上没有经历过真正的推敲和实践就是在自欺欺人。</p>
<h3 id="softmax">Softmax</h3>
<blockquote>
<p>http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/</p>
</blockquote>
<p>Softmax一般会出现在网络的最后一层，用于输出目标类的概率分布，往往用于分类问题。它的定义如下：$ (z)=(_1(z),,_m(z)) $ <span class="math display">\[ \sigma_i(z)=\frac{\exp(z_i)}{\sum_{j=1}^{m}\exp(z_j)}, \quad i=1,\dots,m \]</span> 它在<code>Logistic Regression</code>里起到的作用是<code>线性预测值</code>转化为类别概率：假设 <span class="math inline">\(z_i=w_i^T+b_i\)</span> 是第 <span class="math inline">\(i\)</span> 个类别的线性预测结果，再将其归一化到 <span class="math inline">\((0, 1)\)</span> 区间，现在每个 <span class="math inline">\(o_i=\sigma_i(z)\)</span> 就可以解释成观察到的数据 <span class="math inline">\(x\)</span> 属于类别 <span class="math inline">\(i\)</span> 的概率，或者称作似然 (Likelihood)。</p>
<p>然后Logistic Regression的目标函数是根据最大似然原则来建立的，假设数据 <span class="math inline">\(x\)</span> 所对应的类别为 <span class="math inline">\(y\)</span> ，则根据我们刚才的计算最大似然就是要最大化 <span class="math inline">\(o_y\)</span> 的值(或者说，最小化 <span class="math inline">\(-\log(o_y)\)</span> 也叫做<code>negative log-likelihood</code>)，如下： <span class="math display">\[ l(y, o) = -\log(o_y) \]</span></p>
<p>而<code>Softmax-loss</code>就是将两者结合起来，把 <span class="math inline">\(o_y\)</span> 的定义展开即可： <span class="math display">\[ \tilde{l}(y,z) = -\log\left(\frac{e^{z_y}}{\sum_{j=1}^{m}e^{z_j}} \right) = \log\left(\sum_{j=1}^{m}e^{z_j}\right) - z_y \]</span></p>
<p>没有任何fancy的东西，比如如果我们要写一个Logistic Regression的solver，那么因为要处理的就是这个东西，比较自然地就可以将整个东西合在一起来考虑，或者甚至将<span class="math inline">\(z_i=w_i^T+b_i\)</span>的定义直接一起带进去然后对<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>进行求导来得到Gradient Descent的update rule，例如我们之前介绍Gradient Descent的时候举的两类Logistic Regression的例子就是这样做的。</p>
<h3 id="back-prop">Back Prop</h3>
假设存在一个三层的神经网络，如下图：
<center>
<img src="/image/3nn.png" width="70%">
</center>
<p>除了最开始的数据层 <span class="math inline">\(L^0\)</span> 之外，每一层都有输入节点和输出节点，我们用 <span class="math inline">\(I_2^1\)</span> 表示第一层的第二个输入节点，<span class="math inline">\(O_3^1\)</span> 表示第一层的第三个输出节点，每一层的输入和输出节点数量并不一定要一样多，但通常情况上一层的输出往往就是下一层结点输入的复制，比如 <span class="math inline">\(I_3^2 = O_3^1\)</span>，因为所有的计算都是发生在每一层的内部。所以对普通的神经网络，通常每一层进行的计算都是一个线性映射再加一个<code>activation function</code>，例如<code>sigmoid</code>： <span class="math display">\[ O_i^1 = S(\sum_{j=1}^{3}w_{ij}^{1}I_{j}^{1}+b_i^1) = S\left(\left\langle w_i^1, I^1 \right\rangle+b_i^1\right) \]</span> 现在要对参数 <span class="math inline">\(w_{ij}^1\)</span> 进行求导计算它的gradient来进行gradient descent一类的参数优化，利用链式法则如下： <span class="math display">\[ \frac{\partial{f}}{\partial{w_{ij}^1}} = \sum_{k=1}^{3}\frac{\partial{O_k^1}}{\partial{w_{ij}^1}}\cdot\frac{\partial{f}}{\partial{O_k^1}} \]</span> 注意到 $ / $ 这一项只是和网络的第一层结构有关，只需要知道该结构内部的参数即可计算，而后面一部分相当于 $ / = / $ 即这一层的输出等于下一层的输入。因此，后一项只和下一层的结构和参数有关，不需要知道任何第一层有关的信息。</p>
<p>因此整个网络的参数的gradient的计算方式是从顶层出发，在 <span class="math inline">\(L^p\)</span> 层的时候，会拿到从 <span class="math inline">\(L^{p+1}\)</span> 得到的<span class="math inline">\(\partial{f}/\partial{I^{p+1}}\)</span> 也就是 <span class="math inline">\(\partial{f}/{\partial{O^p}}\)</span>， 然后需要做两个计算：一个是自己层内的参数的gradient，比如是一个不同的<code>full connect layer</code>，则会像上文那样计算，如果没有则继续向下传递。根据链式规则，我们可以计算： <span class="math display">\[ \frac{\partial{f}}{\partial{O_i^{p-1}}} = \frac{\partial{f}}{\partial{I_i^p}} = \sum_{k=1}^{n}\frac{\partial{O_k^p}}{\partial{I_i^p}}\cdot\frac{\partial{f}}{\partial{O_k^p}} \]</span></p>
<h3 id="softmax-loss">Softmax-Loss</h3>
<p>搞清楚Back Prop之后让我们回到Softmax-Loss层，由于该层没有参数，我们只需要计算向后传递的导数就可以了，此外由于该层是最顶层，所以不使用链式法则就可以计算对于最终输出(loss)的导数。我们用 <span class="math inline">\(\tilde{l}(y,z)\)</span> 表示，其中一个是true label <span class="math inline">\(y\)</span>，一个是网络层的输出值 <span class="math inline">\(z\)</span>。由于链式法则的缘故，我们只需要计算 <span class="math inline">\(\partial{\tilde{l}}/\partial{z}\)</span> 然后丢给下一层即可。因此： <span class="math display">\[ \frac{\partial\tilde{l}(y,z)}{\partial{z_k}} = \frac{\exp(z_k)}{\sum_{j=1}^m\exp(z_j)} - \delta_{ky} = \sigma_{k}(z)-\delta_{ky} \]</span> 其中 <span class="math inline">\(\sigma_{k}(z)\)</span> 是Softamx-Loss的中间步骤计算的结果，而 <span class="math inline">\(\delta_{ky}\)</span>，当 <span class="math inline">\(k=y\)</span> 时为 <span class="math inline">\(1\)</span>，当 <span class="math inline">\(k \ne y\)</span> 时为 <span class="math inline">\(0\)</span>，其实就是是否作为变量(常数)对 <span class="math inline">\(z_k\)</span> 求导。</p>

        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </section>

  


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc" data-target="post-toc-wrap">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Kai JIN</p>
  <div class="site-description motion-element" itemprop="description">Life is Wonderful!</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://github.com/atranitell" title="GitHub &rarr; https://github.com/atranitell" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="mailto:atranitell@gmail.com" title="E-Mail &rarr; mailto:atranitell@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
    
      <span class="links-of-author-item">
      
      
      
        
      
        <a href="https://www.facebook.com/atranitell" title="FB Page &rarr; https://www.facebook.com/atranitell" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i></a>
      </span>
    
  </div>



        </div>
      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kai JIN</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.3.0</div>

        












        
      </div>
    </footer>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
<script src="/js/utils.js?v=7.3.0"></script>
<script src="/js/schemes/pisces.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


  
</body>
</html>
